{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2b6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import numpy\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import torch\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from data import ChristmasImages\n",
    "from model import Network\n",
    "\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32161aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\jeeva\\Python\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Users\\jeeva\\Python\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "full_dataset = ChristmasImages('image_data',True)\n",
    "mymodel = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd50c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natsort\n",
      "  Downloading natsort-8.2.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: natsort\n",
      "Successfully installed natsort-8.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f687a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfm = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(degrees=10),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d36873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_ROOT = 'data/train'\n",
    "# #TEST_ROOT = 'data/val'\n",
    "\n",
    "\n",
    "# train_ds = ImageFolder(TRAIN_ROOT, transform=tfm)\n",
    "#test_ds = ImageFolder(TEST_ROOT, transform=tfm)\n",
    "val_size = int(0.2*len(full_dataset))\n",
    "train_size = len(full_dataset)- val_size\n",
    "train_data, val_data = random_split(full_dataset, [train_size, val_size])\n",
    "# valloader = DataLoader(val_data, batch_size=64,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9483a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_TRAIN = len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da873280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3726\n"
     ]
    }
   ],
   "source": [
    "print(LEN_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ca62de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0666ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(full_dataset, batch_size = 64, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbcb5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resnet18(pretrained=True)\n",
    "# # Replace Output of Fully Connected Layer with Number of Labels for our Classification Problem\n",
    "# model.fc = Linear(in_features=512, out_features=8)\n",
    "mymodel = mymodel.to(device)\n",
    "# #model = model.cuda()    # only if your system supports Nvidia CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09a7cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = Adam(mymodel.parameters(), lr=3e-4, weight_decay=0.0001)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "857ad250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 59/59 [14:03<00:00, 14.30s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 14.060747090975443, Loss: 0.7408455014228821\n",
      "Train_acc:0.8011272141706924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 59/59 [13:08<00:00, 13.37s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time: 13.148314352830251, Loss: 0.26147639751434326\n",
      "Train_acc:0.9168008588298443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 59/59 [12:54<00:00, 13.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Time: 12.903110047181448, Loss: 0.3374708294868469\n",
      "Train_acc:0.9514224369296833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 59/59 [13:26<00:00, 13.67s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Time: 13.446826529502868, Loss: 0.5682672262191772\n",
      "Train_acc:0.9626945786366076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 59/59 [14:53<00:00, 15.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Time: 14.890401939551035, Loss: 0.3150326609611511\n",
      "Train_acc:0.9522275899087493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 59/59 [13:58<00:00, 14.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Time: 13.981792962551117, Loss: 0.0917389914393425\n",
      "Train_acc:0.9667203435319377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 59/59 [13:11<00:00, 13.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Time: 13.18790804942449, Loss: 0.007196558173745871\n",
      "Train_acc:0.9817498658078369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 59/59 [12:53<00:00, 13.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Time: 12.899110500017802, Loss: 0.008224870078265667\n",
      "Train_acc:0.9876543209876543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 59/59 [12:44<00:00, 12.96s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Time: 12.747182428836823, Loss: 0.016234751790761948\n",
      "Train_acc:0.9868491680085884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 59/59 [12:56<00:00, 13.16s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Time: 12.939885771274566, Loss: 0.32240626215934753\n",
      "Train_acc:0.9871175523349437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    start = time()\n",
    "    x = 0\n",
    "    tr_acc = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    # Train\n",
    "    mymodel.train()\n",
    "    \n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for xtrain, ytrain in tepoch:\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            xtrain = xtrain.to(device)\n",
    "            ytrain = ytrain.to(device)\n",
    "            train_prob = mymodel(xtrain)\n",
    "            train_prob = train_prob.cpu()\n",
    "            \n",
    "            loss = loss_fn(train_prob, ytrain)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            # training ends\n",
    "            \n",
    "            train_pred = torch.max(train_prob, 1).indices\n",
    "            tr_acc += int(torch.sum(train_pred == ytrain))\n",
    "            \n",
    "        ep_tr_acc = tr_acc / LEN_TRAIN\n",
    "    \n",
    "    # Evaluate\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for xtest, ytest in test_loader:\n",
    "#             xtest = xtest.to(device)\n",
    "#             test_prob = model(xtest)\n",
    "#             test_prob = test_prob.cpu()\n",
    "            \n",
    "#             test_pred = torch.max(test_prob,1).indices\n",
    "#             test_acc += int(torch.sum(test_pred == ytest))\n",
    "            \n",
    "#         ep_test_acc = test_acc / LEN_TEST\n",
    "    \n",
    "    end = time()\n",
    "    duration = (end - start) / 60\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Time: {duration}, Loss: {loss}\\nTrain_acc:{ep_tr_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61dfd5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy  0.9798657718120806\n"
     ]
    }
   ],
   "source": [
    "val_acc = 0\n",
    "for images, labels in valloader:\n",
    "    pred_val_labels = mymodel(images)\n",
    "    val_acc += (pred_val_labels.max(dim=1)[1] == labels).sum().item()\n",
    "print('val accuracy ', val_acc/len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance\n",
    "sample_list= 'data\\\\val\\\\1.png'\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "for image_path in sample_list:\n",
    "    img = Image.open(sample_list)\n",
    "    display(img.resize((224,224)))\n",
    "    img_tensor = tfm(img)\n",
    "    img_tensor = img_tensor[np.newaxis, :]\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    pred_prob = model(img_tensor)\n",
    "    pred = torch.max(pred_prob,1).indices\n",
    "    pred = pred.item()\n",
    "    print(\"========================================================\")\n",
    "# mg_tensor = transform(img)\n",
    "# print(mg_tensor)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir = 'data\\\\val'\n",
    "\n",
    "def mean_std(x):\n",
    "    dataset_mean_std = ImageFolder(x,transform = tfm)\n",
    "    \n",
    "    valloader_mean_std = DataLoader(train_ds_mean_std, batch_size=len(train_ds_mean_std), shuffle=True, num_workers = 1)\n",
    "    \n",
    "    images,labels = next(iter(valloader_mean_std ))\n",
    "    \n",
    "    mean,std = images.mean([0,2,3]), images.std([0,2,3])\n",
    "    \n",
    "    return mean,std\n",
    "\n",
    "#load the train and test data\n",
    "\n",
    "mean,std = mean_std(val_dir)\n",
    "\n",
    "valset = ImageFolder(val_dir,transform = tfm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb5100f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e46d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
